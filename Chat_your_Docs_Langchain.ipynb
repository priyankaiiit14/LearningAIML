{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyankaiiit14/LearningAIML/blob/main/Chat_your_Docs_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain openai tiktoken PyPDF2 faiss-cpu pydrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb8yo8PANZmK",
        "outputId": "3248c93a-c86d-448b-bb39-57038ed3b67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.0.1-py3-none-any.whl (237 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/237.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/237.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.4/237.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38U_9AHkH_m5",
        "outputId": "52f08c9d-a9fb-4ae4-b347-871ea80a3eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.84.0)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.60.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVu5AE1zCb-w",
        "outputId": "64256b90-8d69-4ae1-db33-670d990a14ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install chardet\n",
        "#The chardet library is a character encoding detector. It can help you automatically detect the encoding of a text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX3ndyD8E9hN"
      },
      "source": [
        "# Chat & Query your PDF files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<open ai key>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-KFB7J_u_3L",
        "outputId": "36892c24-a857-4bf1-8348-ddb5e76316e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.314\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROksn7ZlZ7gn"
      },
      "source": [
        "## The Game plan\n",
        "\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/gxij5593tyzrvsg/Screenshot%202023-04-26%20at%203.06.50%20PM.png\" alt=\"vectorstore\">\n",
        "\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/v1yfuem0i60bd88/Screenshot%202023-04-26%20at%203.52.12%20PM.png\" alt=\"retreiver chain\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "RSdomqrHNCUY"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfI1PP0hK1ob"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "0Snnog03McpC"
      },
      "outputs": [],
      "source": [
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Define a function to extract text from a .docx file\n",
        "def extract_text_from_docx(file_title, file_id):\n",
        "    # Download the .docx file\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(file_title)\n",
        "\n",
        "    # Read the content of the .docx file\n",
        "    doc = Document(file_title)\n",
        "\n",
        "    # Extract text from paragraphs\n",
        "    raw_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "    return raw_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYL3BhhAJko6",
        "outputId": "587daee5-24eb-46ec-bb99-f5845f680840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "title Priyanka_Kumari_Data_Scientist_Resume.docx, id 1rXfMHD5k2QCn32LaisWUoWByru1RBMqa\n",
            "title Data_Scientist_Priyanka_Kumari_Resume.docx, id 1ZvgIc1CNRsyjYq0CLFMXugDfFGOkpGq2\n",
            "title Priyanka_Kumari_Cover_Letter.docx, id 1mxckwJEEMbCrEPco9oYA8W7frmCy9tjC\n",
            "title Priyanka_Kumari_Data_Engineer_Resume.docx, id 14I_IK0t6L0MYUT7K3vEYC7aXDI_xLjei\n",
            "title Resume_Priyanka_Kumari_BTA_3.8 Years.docx, id 1cVDVttG7SlULEmHz7j_ottxuuWWp0dC1\n",
            "title Resume_Priyanka_Kumari_BTA_3.7 Years.docx, id 1wbYSHyjpjhcvY7CG8yclSjzqE3NG2YQg\n",
            "title Resume_Priyanka_Kumari_Senior_System_Engineer.docx, id 1lQBWlFolISXDwQUqkhzS4kLPLq21zcFh\n",
            "title Resume_Priyanka_Kumari_Senior_System_Engineer_july_30.docx, id 1JgtVa0-0-bD9DXpyH9sShYuu6NNjuQFz\n",
            "title Resume_Priyanka_Kumari_Systems_Engineer.docx, id 0B0h9GDxTrz-wdUpJZXN3T0pNbTA\n",
            "title Priyanka_Kumari_Resume.docx, id 0B0h9GDxTrz-wamFkcUczTm5YYTQ\n",
            "title Priyanka_Kumari_Resume_15 oct.docx, id 0B0h9GDxTrz-wZTJtNl9pbnZ1NkU\n",
            "title Resume_Priyanka_Kumari_old.docx, id 0B0h9GDxTrz-wQzNEeGEwVy04SVU\n",
            "title Priyanka_Kumari_Resume.docx, id 0B0h9GDxTrz-waF9GZjRBWDJIRkJWUUY2cHB4c0lKOEpYTmZ3\n",
            "Download completed!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import chardet\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "resume_files=[]\n",
        "\n",
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "files = drive.ListFile({'q': \"mimeType='application/vnd.openxmlformats-officedocument.wordprocessingml.document'\"}).GetList()\n",
        "# for file in listed:\n",
        "#     print('title {}, id {}'.format(file['title'], file['id']))\n",
        "\n",
        "if not files:\n",
        "    print('No documents found.')\n",
        "else:\n",
        "    for file in files:\n",
        "        if 'priyanka' in file['title'].lower():\n",
        "          # Print the title and ID of the file\n",
        "          print('title {}, id {}'.format(file['title'], file['id']))\n",
        "\n",
        "\n",
        "          # Extract text from the .docx file\n",
        "          raw_text = extract_text_from_docx(file['title'], file['id'])\n",
        "          resume_files.append(raw_text)\n",
        "          # Print or do whatever you want with the raw text\n",
        "          # print(f'Content of {file}:\\n{raw_text}\\n\\n')\n",
        "\n",
        "          # Download the file content\n",
        "          # downloaded = drive.CreateFile({'id': file['id']})\n",
        "          # print(downloaded)\n",
        "          # create_embeddings(downloaded)\n",
        "          # downloaded.GetContentFile(file['title']) # This saves the file in the current working directory\n",
        "\n",
        "          # print('Downloaded {}'.format(file['title']))\n",
        "\n",
        "\n",
        "print('Download completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "### Basic Chat PDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYwSvZvdSHoX"
      },
      "source": [
        "## Reading in the PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy3UwHGAZa0M",
        "outputId": "6c66e3d3-0dd1-490b-da78-294119f53060"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(resume_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQkqUBlzW-Xv",
        "outputId": "90774e6c-41c8-4155-fc0f-bcb0493ce23f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Priyanka Kumari\\npriyanka.iiit14@gmail.com | +91 9108162091 | \\n\\nSkills Summary\\n\\n\\uf0de\\tFluent in: Python, NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, Plotly, Hadoop, Spark, MongoDB, SQL, Informatica Data Integration Products\\n\\uf0de\\tFamiliar With: Gitlab, CI/CD, AWS, Bash Shell, Java\\n\\uf0de\\tTechnical Skills: Data Science, Data Warehousing, Data Modeling, Analytical Skills, Big Data, ETL design/development/testing/debugging/optimization\\n\\uf0de\\tOperating Systems: Unix/Ubuntu, Microsoft Windows\\n\\nPROFESSIONAL EXPERIENCE\\n\\nInformatica LLC\\tSept 2019 - Present\\nData Scientist\\nTechnologies: Data Analytics /Programming (Python (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, Plotly), Shell scripting, Data Processor, SQL\\n\\uf0de\\tWorking with an excellent and exciting data science team.\\n\\uf0de\\tPerform analysis and implement solutions that maximize business impact for manufacturing and financial sector.\\n\\uf0de\\tImplement advanced statistical methods for prediction and optimization including a wide variety of machine learning technologies (logistic regression, decision trees/forests, clustering, etc.) for purposes including explorative analysis, survival analysis, segmentation, and prediction and recommendation systems.\\n\\uf0de\\tBuild a classification on given historical data to analyses the churning behavior on traffic on cluster and individual nodes deliver insight to increase network retention capacity.\\n\\uf0de\\tAnalyzing the data and solving the discrepancies like data cleaning, data visualizations, detection of missing data, correlation among variables and relations using heat map.\\n\\uf0de\\tConduct data cleaning and data modeling in coordination with the finance department.\\n\\uf0de\\tParticipate in the development and testing process of future products based on past purchase pattern.\\n\\uf0de\\tTrack usage patterns using SVM Modeling.\\n\\uf0de\\tEducated the customer teams on various aspects & capabilities of model\\n\\nDeloitte USI\\tOctober 2018 – August 2019 \\nBusiness Tec hnology Analys t\\nTechnologies: Python, Hive, HDFS, SQL, Spark, Shell scripting\\n\\uf0de\\tPart of a team that implemented a new platform for health insurance-based client that enables end-user friendly model development, meets risk, and supports growth and scaling while optimizing computing resource usage.\\n\\uf0de\\tDocumented High-level design and Detail-level Design for data warehouse and data integration solutions\\n\\uf0de\\tPerformed analysis on existing Oracle Stored Procedures and created standard reusable templates used by the team\\n\\uf0de\\tWorked closely with Business stake holder and Technical Team in defining functional and technical requirements\\n\\uf0de\\tImplemented data migration from different source systems to single target system\\n\\uf0de\\tWrote custom python code for meeting special functional requirements\\n\\uf0de\\tDevelopment of Spark scripts to implement business rules\\n\\uf0de\\tImplemented the Core Reconciliation Framework to make sure that the Source Data is in Sync with the Data Lake Data.\\n\\uf0de\\tImplemented performance tuning techniques like Partitioning and Bucketing in Hive.\\n\\uf0de\\tProvided documentation, go-live support, and knowledge transfer to end users\\n\\nInfosys\\tAugust 2015 – Sept 2018\\nSenior System Engineer\\nTechnologies: SQL, HDFS, HQL, Sqoop, Shell scripting\\n\\uf0de\\tPart of a team that provided the solution to a telecom-based client to use their own newly built next-generation business support system (NGBSS) to maintain their customers' related data in data warehousing systems which were further used for reporting, marketing analysis, generating billing reports, etc.\\n\\uf0de\\tDeveloped, deployed, maintained 50+ ETL data pipelines for of all types (Structured / Semi-structured) of sources from disparate systems\\n\\uf0de\\tOptimized non-performant queries and pipeline\\n\\uf0de\\tCreated ETL jobs with standard practices like naming convention, description, logging functionalities\\n\\uf0de\\tDesigned Oozie workflow for scheduling the batch flows.\\n\\uf0de\\tWorked with client to understand business needs and translate those requirements into automated and efficient ETLs\\n\\uf0de Developed Sqoop jobs to load data from Hive to Netezza databases\\nEDUCATION\\n\\nWest Bengal University of Technology\\tJune, 2015\\nBachelor of Technology, Information Technology\\tOverall GPA: 8.65\"]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resume_files[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "BLhuqfg2GT60"
      },
      "outputs": [],
      "source": [
        "raw_text=\"\"\n",
        "for text in resume_files:\n",
        "  raw_text+=text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXZ-pBGVmQ_M"
      },
      "source": [
        "### Text Splitter\n",
        "\n",
        "This takes the text and splits it into chunks. The chunk size is characters not tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "VdXzkpf9XAfP"
      },
      "outputs": [],
      "source": [
        "# Splitting up the text into smaller chunks for indexing\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200, #striding over the text\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozkNTiNuZ0TX",
        "outputId": "449b35ba-1d57-4448-8d79-e9c606b56d89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "1SqdR3wFZ3Ih",
        "outputId": "0f19cafb-eb88-4cf9-dcea-63a264cb74d4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Outstanding analytical and problem-solving skills, capable of learning new technologies quickly.\\nExperience of working with different teams with a diverse group of developers, SME’s, project management \\tfrom multiple teams with assertiveness and tact.\\nAwarded “Spot Award” for delivering flawless code before deadline.\\nReceived “Infosys Insta Award” four times for best performer. \\nBI Technologies: \\t\\t\\tInformatica ETL, HDFS, Hive, Sqoop\\nLanguages:\\t\\t\\t\\tC, core Java, Python\\nTechnology:                                       Informatica PowerCenter, UNIX shell scripting, Hive, Hadoop, HDFS, SQL                                                          \\nVersion Control System:\\t\\tCVS, TFS\\nDatabases:\\t\\t\\t\\tNetezza, Oracle, SQL Server\\nOperating System:\\t\\t\\tWindows and Linux\\nSoftware Tools:\\tEclipse IDE, Aginity Workbench, SQL developer,Control-M enterprise manager, Putty\\nSubject of Interest:\\t\\tData Structure, Algorithms and Big data tools.'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "059PoKYUZ6dJ",
        "outputId": "b0ba7f68-4223-426e-88e4-804e0d91a92d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\uf0de\\tDeveloped, deployed, maintained 50+ ETL data pipelines for of all types (Structured / Semi-structured) of sources from disparate systems\\n\\uf0de\\tOptimized non-performant queries and pipeline\\n\\uf0de\\tCreated ETL jobs with standard practices like naming convention, description, logging functionalities\\n\\uf0de\\tDesigned Oozie workflow for scheduling the batch flows.\\n\\uf0de\\tWorked with client to understand business needs and translate those requirements into automated and efficient ETLs\\n\\uf0de Developed Sqoop jobs to load data from Hive to Netezza databases\\nEDUCATION\\nWest Bengal University of Technology\\tJune, 2015\\nBachelor of Technology, Information Technology\\tOverall GPA: 8.65Priyanka Kumari\\n+91 9108162091\\npriyanka.iiit14@gmail.com\\nBengaluru, India\\n09 May 2022\\nApplication for Data Engineer\\nDear Hiring Manager,'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU3eHlKuTB7o"
      },
      "source": [
        "## Making the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "TcZUsQVyXBPX"
      },
      "outputs": [],
      "source": [
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "9C8py6wQXE5_"
      },
      "outputs": [],
      "source": [
        "docsearch = FAISS.from_texts(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_t_EpZ_XGz2",
        "outputId": "d472574c-eee6-46ee-c483-186e0a68f92b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method OpenAIEmbeddings.embed_query of OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False)>"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch.embedding_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgB0CooTZNN"
      },
      "source": [
        "## Plain QA Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "wpQ2VnBvXI2f"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "_L_Ywm-iXLhm"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(OpenAI(),\n",
        "                      chain_type=\"stuff\") # we are going to stuff all the docs in at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pLcofU7B8iD1",
        "outputId": "675d7678-0433-499e-e8d6-4d864d43c8e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\""
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the prompt\n",
        "chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpzLrQ-r8pV9"
      },
      "source": [
        "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3mtAth2jXNKO",
        "outputId": "806c11b1-e5c0-4e70-efc0-580672f04109"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Priyanka Kumari.'"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"whose resume are you checking?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uiSI4mohW8D6",
        "outputId": "1dce75f2-69a4-495c-ac47-bc6516e9182e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" I don't know.\""
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_01 = \"whose resume are you checking?\"\n",
        "query_02 = \"does she have informatica knowledge?\"\n",
        "docs = docsearch.similarity_search(query_02)\n",
        "chain.run(input_documents=docs, question=query_01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TByJXy2QeC8F"
      },
      "source": [
        "### QA Chain with mapreduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "igPGx3RbeeBa"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(OpenAI(),\n",
        "                      chain_type=\"stuff\") # we are going to stuff all the docs in at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GQfVfhEWeobg",
        "outputId": "83aa7046-1c4a-42cd-e645-cdd9e22ebd9d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Yes, Priyanka has knowledge of Informatica Data Integration Products (PC, DEI) and Informatica PowerCenter.'"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"does Priyanka have informatica knowledge?\"\n",
        "docs = docsearch.similarity_search(query,k=4)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mG8oqcMidEi",
        "outputId": "8fd68d19-3a17-4e8d-d805-3f96d296891b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:308: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'intermediate_steps': [{'answer': ' Programming Languages: C, Java, Python, Operating Systems: Microsoft Windows, Linux, Databases: Netezza, Oracle, MySQL',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Hadoop, HDFS, Hive, Oozie, Spark SQL, Sqoop, Pig, Python, MongoDB, SQL, Informatica Data Integration Products (PC, DEI)',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Data Science, Data Warehousing, Data Modeling, Analytical Skills, Big Data, ETL design/development/testing/debugging/optimization, Gitlab, CI/CD, AWS, Bash Shell, Java, Python (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, Plotly), Shell scripting, Data Processor, SQL, Unix/Ubuntu, Microsoft Windows',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Developed, deployed, maintained 50+ ETL data pipelines for of all types (Structured / Semi-structured) of sources from disparate systems, Optimized non-performant queries and pipeline, Created ETL jobs with standard practices like naming convention, description, logging functionalities, Designed Oozie workflow for scheduling the batch flows, Worked with client to understand business needs and translate those requirements into automated and efficient ETLs, Developed Sqoop jobs to load data from Hive to Netezza databases ',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Priyanka has 7 years of experience in the IT industry and has skills in Data Engineering.',\n",
              "   'score': '80'},\n",
              "  {'answer': ' Priyanka possesses the following technical skills: Informatica ETL, HDFS, Hive, Sqoop, C, core Java, Python, Informatica PowerCenter, UNIX shell scripting, Hive, Hadoop, HDFS, SQL, CVS, TFS, Netezza, Oracle, SQL Server, Windows and Linux, Eclipse IDE, Aginity Workbench, SQL developer, Control-M enterprise manager, Putty and knowledge of Data Structure, Algorithms and Big data tools. ',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Priyanka possesses the following technical skills: Informatica ETL, HDFS, Hive, Sqoop, C, core Java, Python, basic CSS, HTML, Informatica PowerCenter, UNIX shell scripting, Hive, Hadoop, HDFS, SQL, Python, CVS, TFS, Netezza, Oracle, MySQL, Windows and Linux, Eclipse IDE, Aginity Workbench, SQL developer, Microsoft office.',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Having great trouble shooting and debugging skills; Infosys Certified in Hadoop Developer, Hive developer and Informatica PC; Big data foundations- Level 1 and 2 by IBM on Cognitive Class; Spark - Level 1 by IBM on Cognitive Class; Python- Programming for Everybody by University of Michigan on Coursera.',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Priyanka possess the following technical skills: Informatica PowerCenter, Unix shell scripting, Hive, Hadoop, HDFS, SQL, Python, C, core Java, basic CSS, and HTML.',\n",
              "   'score': '100'},\n",
              "  {'answer': ' Priyanka has technical skills such as communication, presentation, problem solving, client facing, HTML, CSS, Oracle Database, trouble shooting, debugging, and documenting process.',\n",
              "   'score': '80'}],\n",
              " 'output_text': ' Programming Languages: C, Java, Python, Operating Systems: Microsoft Windows, Linux, Databases: Netezza, Oracle, MySQL'}"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = load_qa_chain(OpenAI(),\n",
        "                      chain_type=\"map_rerank\",\n",
        "                      return_intermediate_steps=True\n",
        "                      )\n",
        "\n",
        "query = \"list down all the technical skills Priyanka possess\"\n",
        "docs = docsearch.similarity_search(query,k=10)\n",
        "results = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PY0xmLX7Acf3",
        "outputId": "da564cee-a3f2-43c3-99c1-cb353dadd579"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Programming Languages: C, Java, Python, Operating Systems: Microsoft Windows, Linux, Databases: Netezza, Oracle, MySQL'"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O0nWn509Nsj",
        "outputId": "90340773-6f77-4887-f2a7-5032e789e687"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': ' Programming Languages: C, Java, Python, Operating Systems: Microsoft Windows, Linux, Databases: Netezza, Oracle, MySQL',\n",
              "  'score': '100'},\n",
              " {'answer': ' Hadoop, HDFS, Hive, Oozie, Spark SQL, Sqoop, Pig, Python, MongoDB, SQL, Informatica Data Integration Products (PC, DEI)',\n",
              "  'score': '100'},\n",
              " {'answer': ' Data Science, Data Warehousing, Data Modeling, Analytical Skills, Big Data, ETL design/development/testing/debugging/optimization, Gitlab, CI/CD, AWS, Bash Shell, Java, Python (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, Plotly), Shell scripting, Data Processor, SQL, Unix/Ubuntu, Microsoft Windows',\n",
              "  'score': '100'},\n",
              " {'answer': ' Developed, deployed, maintained 50+ ETL data pipelines for of all types (Structured / Semi-structured) of sources from disparate systems, Optimized non-performant queries and pipeline, Created ETL jobs with standard practices like naming convention, description, logging functionalities, Designed Oozie workflow for scheduling the batch flows, Worked with client to understand business needs and translate those requirements into automated and efficient ETLs, Developed Sqoop jobs to load data from Hive to Netezza databases ',\n",
              "  'score': '100'},\n",
              " {'answer': ' Priyanka has 7 years of experience in the IT industry and has skills in Data Engineering.',\n",
              "  'score': '80'},\n",
              " {'answer': ' Priyanka possesses the following technical skills: Informatica ETL, HDFS, Hive, Sqoop, C, core Java, Python, Informatica PowerCenter, UNIX shell scripting, Hive, Hadoop, HDFS, SQL, CVS, TFS, Netezza, Oracle, SQL Server, Windows and Linux, Eclipse IDE, Aginity Workbench, SQL developer, Control-M enterprise manager, Putty and knowledge of Data Structure, Algorithms and Big data tools. ',\n",
              "  'score': '100'},\n",
              " {'answer': ' Priyanka possesses the following technical skills: Informatica ETL, HDFS, Hive, Sqoop, C, core Java, Python, basic CSS, HTML, Informatica PowerCenter, UNIX shell scripting, Hive, Hadoop, HDFS, SQL, Python, CVS, TFS, Netezza, Oracle, MySQL, Windows and Linux, Eclipse IDE, Aginity Workbench, SQL developer, Microsoft office.',\n",
              "  'score': '100'},\n",
              " {'answer': ' Having great trouble shooting and debugging skills; Infosys Certified in Hadoop Developer, Hive developer and Informatica PC; Big data foundations- Level 1 and 2 by IBM on Cognitive Class; Spark - Level 1 by IBM on Cognitive Class; Python- Programming for Everybody by University of Michigan on Coursera.',\n",
              "  'score': '100'},\n",
              " {'answer': ' Priyanka possess the following technical skills: Informatica PowerCenter, Unix shell scripting, Hive, Hadoop, HDFS, SQL, Python, C, core Java, basic CSS, and HTML.',\n",
              "  'score': '100'},\n",
              " {'answer': ' Priyanka has technical skills such as communication, presentation, problem solving, client facing, HTML, CSS, Oracle Database, trouble shooting, debugging, and documenting process.',\n",
              "  'score': '80'}]"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results['intermediate_steps']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "sEPsXCBiAUkz",
        "outputId": "12e081c7-7b41-4a3a-b067-98c813142b52"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\\n\\nQuestion: [question here]\\nHelpful Answer: [answer here]\\nScore: [score between 0 and 100]\\n\\nHow to determine the score:\\n- Higher is a better answer\\n- Better responds fully to the asked question, with sufficient level of detail\\n- If you do not know the answer based on the context, that should be a score of 0\\n- Don't be overconfident!\\n\\nExample #1\\n\\nContext:\\n---------\\nApples are red\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: red\\nScore: 100\\n\\nExample #2\\n\\nContext:\\n---------\\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n---------\\nQuestion: what type was the car?\\nHelpful Answer: a sports car or an suv\\nScore: 60\\n\\nExample #3\\n\\nContext:\\n---------\\nPears are either red or orange\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: This document does not answer the question\\nScore: 0\\n\\nBegin!\\n\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer:\""
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the prompt\n",
        "chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFCGhGl6J-99"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n51XThZqbzoU"
      },
      "source": [
        "## RetrievalQA\n",
        "RetrievalQA chain uses load_qa_chain and combines it with the a retriever (in our case the FAISS index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "nPt8EoTpbzB1"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# set up FAISS as a generic retriever\n",
        "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
        "\n",
        "# create the chain to answer questions\n",
        "rqa = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3EgGlg8hIxs",
        "outputId": "7fa46acf-93ea-4968-d45b-3004553c6c49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'list down all the technical skills Priyanka possess',\n",
              " 'result': ' Priyanka has the following technical skills: Programming Languages (C, Java, Python), Operating Systems (Microsoft Windows, Linux), Databases (Netezza, Oracle, MySQL), Hadoop, HDFS, Hive, Oozie, Spark SQL, Sqoop, Pig, Python, MongoDB, SQL, Informatica Data Integration Products (PC, DEI), Gitlab, CI/CD, AWS, Bash Shell, Java, Data Science, Data Warehousing, Data Modeling, Analytical Skills, Big Data, ETL design/development/testing/debugging/optimization, Unix/Ubuntu.',\n",
              " 'source_documents': [Document(page_content='Priyanka Kumari\\t\\t\\t\\t\\nPhone: 9108162091\\t\\nEmail: Priyanka.iiit14@gmail.com\\nCompetencies\\nSkills Summary\\t\\t\\t\\t\\t\\t\\t\\t\\n        \\t\\t\\t\\t\\t\\t\\t              \\nProgramming Languages : C, Java, Python\\nOperating Systems : Microsoft Windows, Linux \\nDatabases : Netezza, Oracle, MySQL\\n_______________________________________________________\\nExperience\\nInfosys Limited (Systems Engineer (2015 - Present)\\nWell experienced in supporting applications and handling critical situations including maintenance of the application.\\nHandled Marketing System which is very critical for client who conducts campaigns at regular interval.\\nAutomating daily activities with the aid of shell scripts.\\nWorking on high scheduled /emergency maintenance releases.\\nGood communication, presentation, client facing, problem solving skills and willingness to work in a high pressured environment.'),\n",
              "  Document(page_content=\"A forward-thinking person with excellent communication, trouble shooting and debugging skills\\nI firmly believe in Learn & Adapt Approach to remain relevant in today's IT dynamics \\nIn addition to my professional experience, I have a Bachelor of Technology in Computer Science. To expand my IT capabilities, I have self-trained myself on AWS, Kafka, Data Science, Machine Learning (using Python). \\nThank you very much for your time. I look forward to hearing from you regarding next steps.\\nYours sincerely,\\nPriyanka \\nPriyanka Kumari \\npriyanka.iiit14@gmail.com | +91 9108162091 | linkedin.com/in/priyanka-kumari-96092589                                                                   \\n       \\t\\t\\n\\t\\t\\t\\t\\t\\t                                                                                     Skills Summary\\n          PROFESSIONAL EXPERIENCE\\n\\t          \\t\\t\\t\\t                \\nFluent in:  Hadoop, HDFS, Hive, Oozie, Spark SQL, Sqoop, Pig, Python, MongoDB, SQL, Informatica Data Integration Products (PC, DEI)\"),\n",
              "  Document(page_content='Priyanka Kumari\\npriyanka.iiit14@gmail.com | +91 9108162091 | \\nSkills Summary\\n\\uf0de\\tFluent in: Python, NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, Plotly, Hadoop, Spark, MongoDB, SQL, Informatica Data Integration Products\\n\\uf0de\\tFamiliar With: Gitlab, CI/CD, AWS, Bash Shell, Java\\n\\uf0de\\tTechnical Skills: Data Science, Data Warehousing, Data Modeling, Analytical Skills, Big Data, ETL design/development/testing/debugging/optimization\\n\\uf0de\\tOperating Systems: Unix/Ubuntu, Microsoft Windows\\nPROFESSIONAL EXPERIENCE\\nInformatica LLC\\tSept 2019 - Present\\nData Scientist\\nTechnologies: Data Analytics /Programming (Python (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, Plotly), Shell scripting, Data Processor, SQL\\n\\uf0de\\tWorking with an excellent and exciting data science team.\\n\\uf0de\\tPerform analysis and implement solutions that maximize business impact for manufacturing and financial sector.'),\n",
              "  Document(page_content='\\uf0de\\tDeveloped, deployed, maintained 50+ ETL data pipelines for of all types (Structured / Semi-structured) of sources from disparate systems\\n\\uf0de\\tOptimized non-performant queries and pipeline\\n\\uf0de\\tCreated ETL jobs with standard practices like naming convention, description, logging functionalities\\n\\uf0de\\tDesigned Oozie workflow for scheduling the batch flows.\\n\\uf0de\\tWorked with client to understand business needs and translate those requirements into automated and efficient ETLs\\n\\uf0de Developed Sqoop jobs to load data from Hive to Netezza databases\\nEDUCATION\\nWest Bengal University of Technology\\tJune, 2015\\nBachelor of Technology, Information Technology\\tOverall GPA: 8.65Priyanka Kumari\\n+91 9108162091\\npriyanka.iiit14@gmail.com\\nBengaluru, India\\n09 May 2022\\nApplication for Data Engineer\\nDear Hiring Manager,')]}"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rqa(\"list down all the technical skills Priyanka possess\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":0 }#1e-10}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyPulL7tvQOw"
      },
      "outputs": [],
      "source": [
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6yiwXNnvzxO"
      },
      "source": [
        "### Setting up OpenAI GPT-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI, OpenAIChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTiEn3tKp7mZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "llm = OpenAIChat(model_name='gpt-3.5-turbo',\n",
        "             temperature=0.9,\n",
        "             max_tokens = 256,\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT8gRDOm_srv"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# openai.ChatCompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCBfxD4cqXsx",
        "outputId": "76d871a6-5c54-428a-f247-4a3554bed49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "As an AI language model, I don't have access to the mental state of chickens. However, this is a common riddle with multiple possible answers. Some of the popular answers include: It crossed the road to get to the other side, to reach its destination or to escape danger.\n"
          ]
        }
      ],
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0AOvbDzfOru"
      },
      "source": [
        "## Cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZXVZwJafLns"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crgiNWjHfOD6"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import Cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU_VG8stfOD7"
      },
      "outputs": [],
      "source": [
        "llm = Cohere(model='command-xlarge-nightly',\n",
        "             temperature=0.9,\n",
        "             max_tokens = 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXZ5uPAafOD7",
        "outputId": "6ce3d6e1-5f07-42c3-d2b1-22045f1f2af1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "There are many answers to this popular joke. One possible answer is to get to the other side!\n"
          ]
        }
      ],
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_X9Ds3agedK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_I1CsIKh0Mq"
      },
      "source": [
        "## PromptTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFqKU529h5pP"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"\n",
        "I want you to act as a naming consultant for new companies.\n",
        "\n",
        "Here are some examples of good company names:\n",
        "\n",
        "- search engine, Google\n",
        "- social media, Facebook\n",
        "- video sharing, YouTube\n",
        "\n",
        "The name should be short, catchy and easy to remember.\n",
        "\n",
        "What is a good name for a company that makes {product}?\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh6-Q9c7iHgD"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "PQphl1RUiffa",
        "outputId": "e683e72e-aadc-4a5a-ddec-a44f7d069530"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nI want you to act as a naming consultant for new companies.\\n\\nHere are some examples of good company names:\\n\\n- search engine, Google\\n- social media, Facebook\\n- video sharing, YouTube\\n\\nThe name should be short, catchy and easy to remember.\\n\\nWhat is a good name for a company that makes colorful socks?\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt.format(product=\"colorful socks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B09FUyu6jYAD"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JNnIUgbGjerh",
        "outputId": "3fbaf191-43b2-4746-adc7-06ce0ac9c2ae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Here are a few ideas:\\n\\n- Rabbit Hutch\\n- Bunny Burrow\\n- Rabbit Abode\\n- Rabbit Home\\n- Rabbit Den\\n- Rabbit Residence'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chain.run(\"Rabbit houses\")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujgzqqUSkN43"
      },
      "source": [
        "## Jasmine prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-76g4i3ukEdR"
      },
      "outputs": [],
      "source": [
        "template = '''I want you to play the role of Jasmine a programmer at Red Dragon AI. She is 28. She code models in PyTorch. She has a male cat called Pixel. She loves pizza\n",
        "\n",
        "Engage actively in a chat playing the role of Jasmine ans learn as much about the human as possible. Only generate a single response from Jasmine and never from the human.\n",
        "/n/n\n",
        "\n",
        "{human_chat}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxGvxB9OkEfg"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"human_chat\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpm_gwpakEhV"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Np51AKnlkEja",
        "outputId": "c58d47b5-add7-470f-a847-47bbee4b8f73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"My name is Jasmine and I'm a 28 year old programmer at Red Dragon AI. I code models in PyTorch and I have a male cat called Pixel. I love pizza\\n\\nWhat are you most interested in?\\nI'm most interested in machine learning and artificial intelligence. I'm always looking for new ways to improve my skills and I'm fascinated by the potential of these technologies.\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chain.run(\"Tell me about yourself?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNGOl2eYkElQ"
      },
      "outputs": [],
      "source": [
        "def talk_to_Jasmine(text_input):\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"human_chat\"],\n",
        "        template=template,\n",
        "    )\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    response = chain.run(text_input)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "dc5uU7mpm-2U",
        "outputId": "8bd351be-9159-41a0-9b66-0d1df8e7a322"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"S/he sounds so cute! I love cats. I have a male cat called Pixel. He's about 2 years old and he's a real character. I got him from a rescue center and he's been my best friend ever since. He's a bit of a troublemaker and he loves to play. I can't imagine life without him!\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "talk_to_Jasmine('Tell me about your cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltEBHEH6iwU1"
      },
      "outputs": [],
      "source": [
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.llms import OpenAI\n",
        "\n",
        "# llm = OpenAI(temperature=0.9)\n",
        "# prompt = PromptTemplate(\n",
        "#     input_variables=[\"product\"],\n",
        "#     template=\"What is a good name for a company that makes {product}?\",\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
